{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_eval_loss = []\n",
    "instruct_train_loss = []\n",
    "path_to_save_metrics = \"../../../results/llama3_results/instruct/metrics\"\n",
    "steps = []\n",
    "with open(path_to_save_metrics + \"/trainer_state.json\") as f:\n",
    "    instruct_metrics = json.load(f)\n",
    "    log_history = instruct_metrics[\"log_history\"]\n",
    "\n",
    "    for log in log_history:\n",
    "        if \"eval_loss\" in log:\n",
    "            instruct_eval_loss.append(log[\"eval_loss\"])\n",
    "            steps.append(log[\"step\"])\n",
    "        elif \"loss\" in log:\n",
    "            instruct_train_loss.append(log[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(instruct_eval_loss) == len(instruct_train_loss), \"Length must match\"\n",
    "assert len(instruct_eval_loss) == len(steps), \"Length must match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruct finetuning metrics\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(steps, instruct_eval_loss, \"r-\", label=\"Evaluation loss\")\n",
    "ax.plot(steps, instruct_train_loss, \"g-\", label=\"Train loss\")\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"Loss\", color=\"black\")\n",
    "ax.legend()\n",
    "plt.title(\"Llama 3 instruct\")\n",
    "txt = \"lr=2.0e-05, batch_size=4, epochs=1, gradient_clip=1.0 \\n Training duration: 370 minutes\"\n",
    "plt.figtext(0.5, -0.05, txt, wrap=True, horizontalalignment=\"center\", fontsize=12)\n",
    "\n",
    "plt.savefig(path_to_save_metrics + \"/graph_1_epoch.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Scripts.llama_model_wrapper import InstructModelWrapper\n",
    "from Scripts.load_dataset import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    \"path\": \"../../../results/llama3_results/instruct/model.nosync\",\n",
    "    \"tokenizer_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    \"device_map\": \"auto\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model_wrapper = InstructModelWrapper(**model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_ds = load_dataset(\n",
    "    \"../../../../German_newspaper_articles/10kGNAD/train.csv\",\n",
    "    \"../../../../German_newspaper_articles/10kGNAD/test.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_ds.map(\n",
    "    instruct_model_wrapper.create_test_messages, remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_ds.map(instruct_model_wrapper.tokenize_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.set_format(\"torch\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "correct_dict = {\n",
    "    \"Web\": 0,\n",
    "    \"International\": 0,\n",
    "    \"Etat\": 0,\n",
    "    \"Wirtschaft\": 0,\n",
    "    \"Panorama\": 0,\n",
    "    \"Sport\": 0,\n",
    "    \"Wissenschaft\": 0,\n",
    "    \"Kultur\": 0,\n",
    "    \"Inland\": 0,\n",
    "}\n",
    "y_true = []\n",
    "y_pred = []\n",
    "wrong = []\n",
    "instruct_model_wrapper.model.eval()\n",
    "for i, sample in enumerate(test_ds):\n",
    "    outputs = instruct_model_wrapper.model.generate(\n",
    "        sample[\"input_ids\"],\n",
    "        max_new_tokens=128,\n",
    "        eos_token_id=instruct_model_wrapper.terminators,\n",
    "        do_sample=False,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][sample[\"input_ids\"].shape[-1] :]\n",
    "    response = instruct_model_wrapper.tokenizer.decode(\n",
    "        response, skip_special_tokens=True\n",
    "    )\n",
    "    y_true.append(sample[\"label\"])\n",
    "    y_pred.append(response)\n",
    "    if sample[\"label\"] in response:\n",
    "        correct += 1\n",
    "        correct_dict[sample[\"label\"]] += 1\n",
    "        print(f\"At {i}: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(y_true)) == len(set(y_pred)), \"Labels are not the same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = Counter(test_ds[\"label\"])\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(label_counts.keys())\n",
    "differences = dict()\n",
    "for label in labels:\n",
    "    differences[label] = correct_dict[label] / label_counts[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_metrics = \"../../../results/llama3_results/instruct/metrics\"\n",
    "\n",
    "difference_values = [value * 100 for value in differences.values()]\n",
    "difference_labels = [value for value in differences.keys()]\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "xs = range(len(difference_labels))\n",
    "ys = [difference_values[x] for x in xs]\n",
    "\n",
    "ax.bar(difference_labels, ys, 0.6)\n",
    "plt.title(\"correct per category\")\n",
    "plt.xlabel(\"category\")\n",
    "plt.ylabel(\"accuracy in %\")\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment=\"right\")\n",
    "plt.savefig(path_to_save_metrics + \"/test_evaluation.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    labels=[\n",
    "        \"Web\",\n",
    "        \"International\",\n",
    "        \"Etat\",\n",
    "        \"Wirtschaft\",\n",
    "        \"Panorama\",\n",
    "        \"Sport\",\n",
    "        \"Wissenschaft\",\n",
    "        \"Kultur\",\n",
    "        \"Inland\",\n",
    "    ],\n",
    ")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true, y_pred, average=\"weighted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv.nosync",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
